{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0824f5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d796b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    " # 1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_wikipedia_headers(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all header tags (h1 to h6)\n",
    "        header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        \n",
    "        # Extract the text content of each header tag\n",
    "        headers_text = [tag.text.strip() for tag in header_tags]\n",
    "        \n",
    "        # Create a DataFrame with the headers\n",
    "        df = pd.DataFrame({'Headers': headers_text})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch content from {url}. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "wikipedia_headers_df = get_wikipedia_headers(url)\n",
    "print(wikipedia_headers_df)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a26a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to fetch content from https://presidentofindia.nic.in/former-presidents.htm. Status Code: 404\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "# from https://presidentofindia.nic.in/former-presidents.htm and make data frame.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_former_presidents(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the container containing the information about former presidents\n",
    "        presidents_container = soup.find('div', class_='field-item even')\n",
    "        \n",
    "        # Find all the rows (tr) in the table\n",
    "        rows = presidents_container.find_all('tr')\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        names = []\n",
    "        terms = []\n",
    "        \n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows[1:]:  # Skip the header row\n",
    "            columns = row.find_all('td')\n",
    "            name = columns[0].text.strip()\n",
    "            term = columns[1].text.strip()\n",
    "            \n",
    "            # Append data to lists\n",
    "            names.append(name)\n",
    "            terms.append(term)\n",
    "        \n",
    "        # Create a DataFrame with the extracted data\n",
    "        df = pd.DataFrame({'Name': names, 'Term of Office': terms})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch content from {url}. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "former_presidents_df = get_former_presidents(url)\n",
    "print(former_presidents_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0370b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "               Team Matches Points Rating\n",
      "0        India\\nIND      55  6,640    121\n",
      "1    Australia\\nAUS      42  4,926    117\n",
      "2  South Africa\\nSA      34  3,750    110\n",
      "3     Pakistan\\nPAK      36  3,922    109\n",
      "4   New Zealand\\nNZ      43  4,399    102\n",
      "5      England\\nENG      38  3,777     99\n",
      "6     Sri Lanka\\nSL      47  4,134     88\n",
      "7   Bangladesh\\nBAN      44  3,836     87\n",
      "8  Afghanistan\\nAFG      30  2,533     84\n",
      "9   West Indies\\nWI      38  2,582     68\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "                 Batsman Team Rating\n",
      "0           Shubman Gill  IND    826\n",
      "1             Babar Azam  PAK    824\n",
      "2            Virat Kohli  IND    791\n",
      "3           Rohit Sharma  IND    769\n",
      "4        Quinton de Kock   SA    760\n",
      "5         Daryl Mitchell   NZ    750\n",
      "6           David Warner  AUS    745\n",
      "7  Rassie van der Dussen   SA    735\n",
      "8           Harry Tector  IRE    729\n",
      "9            Dawid Malan  ENG    729\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "           Bowler Team Rating\n",
      "0  Keshav Maharaj   SA    741\n",
      "1  Josh Hazlewood  AUS    703\n",
      "2  Mohammed Siraj  IND    699\n",
      "3  Jasprit Bumrah  IND    685\n",
      "4      Adam Zampa  AUS    675\n",
      "5     Rashid Khan  AFG    667\n",
      "6   Kuldeep Yadav  IND    667\n",
      "7     Trent Boult   NZ    663\n",
      "8  Shaheen Afridi  PAK    650\n",
      "9  Mohammad Shami  IND    648\n"
     ]
    }
   ],
   "source": [
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data framea) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "# b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "# c) Top 10 ODI bowlers along with the records of their team andrating.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_odi_team_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the container containing the information about ODI team rankings\n",
    "        rankings_container = soup.find('table', class_='table')\n",
    "        \n",
    "        # Find all the rows (tr) in the table\n",
    "        rows = rankings_container.find_all('tr')\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        teams = []\n",
    "        matches = []\n",
    "        points = []\n",
    "        ratings = []\n",
    "        \n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows[1:11]:  # Extract data for the top 10 teams\n",
    "            columns = row.find_all('td')\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            \n",
    "            # Append data to lists\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        # Create a DataFrame with the extracted data\n",
    "        df = pd.DataFrame({'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch content from {url}. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_odi_batsmen_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the container containing the information about ODI batsmen rankings\n",
    "        rankings_container = soup.find('table', class_='table rankings-table')\n",
    "        \n",
    "        # Find all the rows (tr) in the table\n",
    "        rows = rankings_container.find_all('tr')\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        batsmen = []\n",
    "        teams = []\n",
    "        ratings = []\n",
    "        \n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows[1:11]:  # Extract data for the top 10 batsmen\n",
    "            columns = row.find_all('td')\n",
    "            batsman = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            # Append data to lists\n",
    "            batsmen.append(batsman)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        # Create a DataFrame with the extracted data\n",
    "        df = pd.DataFrame({'Batsman': batsmen, 'Team': teams, 'Rating': ratings})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch content from {url}. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_odi_bowlers_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the container containing the information about ODI bowlers rankings\n",
    "        rankings_container = soup.find('table', class_='table rankings-table')\n",
    "        \n",
    "        # Find all the rows (tr) in the table\n",
    "        rows = rankings_container.find_all('tr')\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        bowlers = []\n",
    "        teams = []\n",
    "        ratings = []\n",
    "        \n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows[1:11]:  # Extract data for the top 10 bowlers\n",
    "            columns = row.find_all('td')\n",
    "            bowler = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            # Append data to lists\n",
    "            bowlers.append(bowler)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        # Create a DataFrame with the extracted data\n",
    "        df = pd.DataFrame({'Bowler': bowlers, 'Team': teams, 'Rating': ratings})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch content from {url}. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "odi_teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "odi_batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "odi_bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "\n",
    "odi_teams_df = get_odi_team_rankings(odi_teams_url)\n",
    "odi_batsmen_df = get_odi_batsmen_rankings(odi_batsmen_url)\n",
    "odi_bowlers_df = get_odi_bowlers_rankings(odi_bowlers_url)\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(odi_teams_df)\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(odi_batsmen_df)\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(odi_bowlers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65dc9b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Teams:\n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,429    163\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      20  1,768     88\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      14  1,074     77\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      24  1,602     67\n",
      "\n",
      "Top 10 Women's ODI Batting Players:\n",
      "                 Player Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    807\n",
      "1           Beth Mooney  AUS    750\n",
      "2   Chamari Athapaththu   SL    736\n",
      "3       Laura Wolvaardt   SA    727\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    698\n",
      "6          Ellyse Perry  AUS    697\n",
      "7      Harmanpreet Kaur  IND    694\n",
      "8           Meg Lanning  AUS    662\n",
      "9        Marizanne Kapp   SA    642\n",
      "\n",
      "Top 10 Women's ODI All-Rounders:\n",
      "            All-Rounder Team Rating\n",
      "0        Marizanne Kapp   SA    385\n",
      "1      Ashleigh Gardner  AUS    377\n",
      "2  Natalie Sciver-Brunt  ENG    360\n",
      "3       Hayley Matthews   WI    358\n",
      "4           Amelia Kerr   NZ    346\n",
      "5         Deepti Sharma  IND    312\n",
      "6          Ellyse Perry  AUS    282\n",
      "7              Nida Dar  PAK    240\n",
      "8         Jess Jonassen  AUS    227\n",
      "9         Sophie Devine   NZ    227\n"
     ]
    }
   ],
   "source": [
    "# 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data framea) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_womens_odi_team_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the container containing the information about Women's ODI team rankings\n",
    "        rankings_container = soup.find('table', class_='table')\n",
    "        \n",
    "        # Find all the rows (tr) in the table\n",
    "        rows = rankings_container.find_all('tr')\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        teams = []\n",
    "        matches = []\n",
    "        points = []\n",
    "        ratings = []\n",
    "        \n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows[1:11]:  # Extract data for the top 10 teams\n",
    "            columns = row.find_all('td')\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            \n",
    "            # Append data to lists\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        # Create a DataFrame with the extracted data\n",
    "        df = pd.DataFrame({'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch content from {url}. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_womens_odi_batting_players_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the container containing the information about Women's ODI batting players rankings\n",
    "        rankings_container = soup.find('table', class_='table rankings-table')\n",
    "        \n",
    "        # Find all the rows (tr) in the table\n",
    "        rows = rankings_container.find_all('tr')\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        players = []\n",
    "        teams = []\n",
    "        ratings = []\n",
    "        \n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows[1:11]:  # Extract data for the top 10 batting players\n",
    "            columns = row.find_all('td')\n",
    "            player = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            # Append data to lists\n",
    "            players.append(player)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        # Create a DataFrame with the extracted data\n",
    "        df = pd.DataFrame({'Player': players, 'Team': teams, 'Rating': ratings})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch content from {url}. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_womens_odi_allrounders_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the container containing the information about Women's ODI all-rounders rankings\n",
    "        rankings_container = soup.find('table', class_='table rankings-table')\n",
    "        \n",
    "        # Find all the rows (tr) in the table\n",
    "        rows = rankings_container.find_all('tr')\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        allrounders = []\n",
    "        teams = []\n",
    "        ratings = []\n",
    "        \n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows[1:11]:  # Extract data for the top 10 all-rounders\n",
    "            columns = row.find_all('td')\n",
    "            allrounder = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            # Append data to lists\n",
    "            allrounders.append(allrounder)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        # Create a DataFrame with the extracted data\n",
    "        df = pd.DataFrame({'All-Rounder': allrounders, 'Team': teams, 'Rating': ratings})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch content from {url}. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "womens_odi_teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "womens_odi_batting_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "womens_odi_allrounders_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "\n",
    "womens_odi_teams_df = get_womens_odi_team_rankings(womens_odi_teams_url)\n",
    "womens_odi_batting_df = get_womens_odi_batting_players_rankings(womens_odi_batting_url)\n",
    "womens_odi_allrounders_df = get_womens_odi_allrounders_rankings(womens_odi_allrounders_url)\n",
    "\n",
    "print(\"Top 10 Women's ODI Teams:\")\n",
    "print(womens_odi_teams_df)\n",
    "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "print(womens_odi_batting_df)\n",
    "print(\"\\nTop 10 Women's ODI All-Rounders:\")\n",
    "print(womens_odi_allrounders_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e44f0844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "# make data frame\n",
    "# i) Headline\n",
    "# ii) Time\n",
    "# iii) News Link\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_cnbc_world_news(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the container containing the information about news articles\n",
    "        news_container = soup.find('div', class_='Card-titleContainer')\n",
    "        \n",
    "        # Find all the news articles\n",
    "        news_articles = news_container.find_all('div', class_='Card-title')\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        headlines = []\n",
    "        times = []\n",
    "        news_links = []\n",
    "        \n",
    "        # Loop through each news article and extract the data\n",
    "        for article in news_articles:\n",
    "            # Extract headline\n",
    "            headline = article.find('a').text.strip()\n",
    "            headlines.append(headline)\n",
    "            \n",
    "            # Extract time\n",
    "            time = article.find('time').text.strip()\n",
    "            times.append(time)\n",
    "            \n",
    "            # Extract news link\n",
    "            news_link = article.find('a')['href'].strip()\n",
    "            news_links.append(news_link)\n",
    "        \n",
    "        # Create a DataFrame with the extracted data\n",
    "        df = pd.DataFrame({'Headline': headlines, 'Time': times, 'News Link': news_links})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch content from {url}. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "cnbc_world_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "cnbc_world_news_df = get_cnbc_world_news(cnbc_world_url)\n",
    "print(cnbc_world_news_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5febf635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 6) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "# days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "# Scrape below mentioned details and make data framei) Paper Title\n",
    "# ii) Authors\n",
    "# iii) Published Date\n",
    "# iv) Paper URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        titles = []\n",
    "        authors = []\n",
    "        published_dates = []\n",
    "        paper_urls = []\n",
    "\n",
    "        articles = soup.find_all('div', class_='pod-listing-header')\n",
    "\n",
    "        for article in articles:\n",
    "            title = article.find('a', class_='pod-listing-header__title-link')\n",
    "            titles.append(title.text.strip() if title else 'N/A')\n",
    "\n",
    "            author = article.find('span', class_='pod-listing-header__authors')\n",
    "            authors.append(author.text.strip() if author else 'N/A')\n",
    "\n",
    "            date = article.find('span', class_='pod-listing-header__date')\n",
    "            published_dates.append(date.text.strip() if date else 'N/A')\n",
    "\n",
    "            link = article.find('a', class_='pod-listing-header__title-link')\n",
    "            paper_urls.append(link['href'].strip() if link else 'N/A')\n",
    "\n",
    "        data = {'Paper Title': titles, 'Authors': authors, 'Published Date': published_dates, 'Paper URL': paper_urls}\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "    df = scrape_most_downloaded_articles(url)\n",
    "\n",
    "    if df is not None:\n",
    "        print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597c05e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Restaurant Name Cuisine Location Ratings Image URL\n",
      "0              N/A     N/A      N/A     N/A       N/A\n",
      "1              N/A     N/A      N/A     N/A       N/A\n",
      "2              N/A     N/A      N/A     N/A       N/A\n",
      "3              N/A     N/A      N/A     N/A       N/A\n",
      "4              N/A     N/A      N/A     N/A       N/A\n",
      "5              N/A     N/A      N/A     N/A       N/A\n",
      "6              N/A     N/A      N/A     N/A       N/A\n",
      "7              N/A     N/A      N/A     N/A       N/A\n",
      "8              N/A     N/A      N/A     N/A       N/A\n",
      "9              N/A     N/A      N/A     N/A       N/A\n",
      "10             N/A     N/A      N/A     N/A       N/A\n",
      "11             N/A     N/A      N/A     N/A       N/A\n",
      "12             N/A     N/A      N/A     N/A       N/A\n",
      "13             N/A     N/A      N/A     N/A       N/A\n",
      "14             N/A     N/A      N/A     N/A       N/A\n",
      "15             N/A     N/A      N/A     N/A       N/A\n",
      "16             N/A     N/A      N/A     N/A       N/A\n",
      "17             N/A     N/A      N/A     N/A       N/A\n",
      "18             N/A     N/A      N/A     N/A       N/A\n",
      "19             N/A     N/A      N/A     N/A       N/A\n",
      "20             N/A     N/A      N/A     N/A       N/A\n"
     ]
    }
   ],
   "source": [
    "# 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\n",
    "# i) Restaurant name\n",
    "# ii) Cuisine\n",
    "# iii) Location\n",
    "# iv) Ratings\n",
    "# v) Image URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        restaurant_names = []\n",
    "        cuisines = []\n",
    "        locations = []\n",
    "        ratings = []\n",
    "        image_urls = []\n",
    "\n",
    "        restaurants = soup.find_all('div', class_='restnt-info')\n",
    "        for restaurant in restaurants:\n",
    "            # Restaurant Name\n",
    "            name = restaurant.find('div', class_='restnt-info__name')\n",
    "            restaurant_names.append(name.text.strip() if name else 'N/A')\n",
    "\n",
    "            # Cuisine\n",
    "            cuisine = restaurant.find('div', class_='restnt-info__detail')\n",
    "            cuisines.append(cuisine.text.strip() if cuisine else 'N/A')\n",
    "\n",
    "            # Location\n",
    "            location = restaurant.find('div', class_='restnt-info__add')\n",
    "            locations.append(location.text.strip() if location else 'N/A')\n",
    "\n",
    "            # Ratings\n",
    "            rating = restaurant.find('div', class_='restnt-rating__value')\n",
    "            ratings.append(rating.text.strip() if rating else 'N/A')\n",
    "\n",
    "            # Image URL\n",
    "            image = restaurant.find('div', class_='restnt-thumb__img')\n",
    "            image_url = image.find('img')['src'] if image else 'N/A'\n",
    "            image_urls.append(image_url)\n",
    "\n",
    "        data = {'Restaurant Name': restaurant_names, 'Cuisine': cuisines, 'Location': locations, 'Ratings': ratings, 'Image URL': image_urls}\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "    df = scrape_dineout_details(url)\n",
    "\n",
    "    if df is not None:\n",
    "        print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bf6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
